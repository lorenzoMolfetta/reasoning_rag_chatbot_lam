# ğŸ¤– Deploy di un Chatbot LAM con AutoGen e Ollama

## ğŸ“‹ Panoramica

In questa lezione analizziamo come creare e fare deployment di un chatbot LAM-based (Large Action Model) utilizzando AutoGen Studio e Ollama.

## ğŸ¯ Cosa Costruiremo

- Un chatbot LAM completamente funzionale
- Integrazione con modelli locali tramite Ollama
- Interfaccia web tramite AutoGen Studio
- CapacitÃ  di ricerca web tramite Google Custom Search API

---

## ğŸ› ï¸ Prerequisiti e Installazione

### 1. ğŸ“¥ Installazione di Ollama

Ollama Ã¨ il runtime che ci permetterÃ  di eseguire modelli di AI in locale.

**Passo 1:** Scarica e installa Ollama
- Vai su https://ollama.com/
- Scarica la versione per il tuo sistema operativo
- Segui le istruzioni di installazione

**Passo 2:** Avvia il servizio Ollama
```bash
# Apri un nuovo terminale e avvia Ollama in background
ollama serve >/dev/null 2>&1 &
```

**Passo 3:** Scarica i modelli necessari
```bash
# Scarica il modello Qwen3 (leggero e veloce)
ollama pull qwen3:4b

# Opzionale: scarica anche Llama3.2 per avere piÃ¹ opzioni
ollama pull llama3.2:3b
```

---

## ğŸ”‘ Configurazione delle Credenziali Google

Per abilitare le funzionalitÃ  di ricerca web, dovrai configurare l'API di Google Custom Search.

### Parte 1: ğŸ” Ottenere la Google API Key

#### Passo 1: Accesso alla Google Cloud Console
- ğŸŒ Vai su https://console.cloud.google.com/
- ğŸ‘¤ Accedi con il tuo account Google

#### Passo 2: Gestione del Progetto
- ğŸ†• **Se non hai progetti:** Clicca su "Crea progetto", inserisci un nome significativo (es. "AutoGen-LAM-Bot") e clicca "Crea"
- ğŸ“ **Se hai giÃ  progetti:** Seleziona quello che vuoi utilizzare per questo chatbot

#### Passo 3: Abilitazione dell'API
- ğŸ“š Nel menu laterale: "API e servizi" â†’ "Libreria"
- ğŸ” Cerca "Custom Search API"
- âœ… Seleziona l'API e premi "Abilita"

#### Passo 4: Creazione delle Credenziali
- ğŸ”‘ Vai su "API e servizi" â†’ "Credenziali"
- â• Clicca "Crea credenziali" â†’ "Chiave API"
- ğŸ’¾ **IMPORTANTE:** Copia e salva la chiave generata in un posto sicuro

### Parte 2: ğŸ” Ottenere il Google CSE ID

#### Passo 1: Accesso al Programmable Search Engine
- ğŸŒ Vai su https://programmablesearchengine.google.com/
- ğŸ‘¤ Accedi con lo stesso account Google utilizzato per l'API

#### Passo 2: Creazione del Motore di Ricerca
- â• Clicca su "Aggiungi"

#### Passo 3: Configurazione Avanzata
**Opzioni per "Siti da cercare":**
- ğŸŒ **Raccomandato:** Seleziona "Ricerca in tutto il web" per massima flessibilitÃ 
- ğŸ¯ **Alternativa:** Inserisci domini specifici (es. wikipedia.org, stackoverflow.com)

#### Passo 4: Finalizzazione
- âœ… Clicca su "Crea"
- ğŸ“‹ Annota il **Search Engine ID** che viene generato

![CSE ID Creation](assets/cse_id_creation.png)
*Figura: Processo di creazione del Custom Search Engine ID*

---

## ğŸ³ Deploy con Docker

### Configurazione del Container AutoGen

Creiamo l'immagine Docker con:
```bash
docker build -f build/Dockerfile -t autogen_image .
```

Utilizza il seguente comando per avviare AutoGen Studio in un container Docker:
```bash
docker run -d \
       --name autogen \
       -w /app \
       -v $(pwd):/app \
       -e GOOGLE_API_KEY="LA_TUA_API_KEY_QUI" \
       -e GOOGLE_CSE_ID="IL_TUO_CSE_ID_QUI" \
       -p 40000:8080 \
       autogen_image:latest \
       autogenstudio ui --host 0.0.0.0 --port 8080 --appdir /app/my_app
```

**âš ï¸ IMPORTANTE:** Sostituisci `LA_TUA_API_KEY_QUI` e `IL_TUO_CSE_ID_QUI` con le credenziali ottenute nei passaggi precedenti.

### ğŸŒ Accesso all'Interfaccia Web

Dopo aver avviato il container:
- ğŸ–¥ï¸ Apri il browser e vai su http://localhost:40000
- ğŸ‰ Dovresti vedere l'interfaccia di AutoGen Studio

---

## âš™ï¸ Configurazione dei Modelli

### 1. ğŸ“ Aggiunta di Modelli Locali

Nell'interfaccia di AutoGen Studio:

**Navigazione:** Gallery â†’ Models â†’ "+ Add Model" â†’ "Json Editor"

### 2. ğŸ¤– Configurazione Modello Qwen3

Copia e incolla questa configurazione:

```json
{
  "provider": "autogen_ext.models.openai.OpenAIChatCompletionClient",
  "component_type": "model",
  "version": 1,
  "component_version": 1,
  "description": "Local Ollama Qwen3-0.6b model client per generazione basata su istruzioni.",
  "label": "Qwen3 Model",
  "config": {
    "model": "qwen3:4b",
    "api_key": "ollama",
    "model_info": {
      "vision": false,
      "structured_output": true,
      "function_calling": true,
      "json_output": false,
      "family": "unknown"
    },
    "base_url": "http://host.docker.internal:11434/v1"
  }
}
```

### 3. ğŸ¦™ Configurazione Modello Llama3.2 (Opzionale)

```json
{
  "provider": "autogen_ext.models.openai.OpenAIChatCompletionClient",
  "component_type": "model",
  "version": 1,
  "component_version": 1,
  "description": "Local Ollama Llama3.2 model client per generazione avanzata.",
  "label": "Llama3.2 Model",
  "config": {
    "model": "llama3.2:3b",
    "api_key": "ollama",
    "model_info": {
      "vision": false,
      "structured_output": true,
      "function_calling": true,
      "json_output": false,
      "family": "unknown"
    },
    "base_url": "http://host.docker.internal:11434/v1"
  }
}
```


![Ollama Model Creation](assets/agent_model_creation.png)
*Figura: Interfaccia di configurazione del modello Ollama negli agenti - mostra come inserire la configurazione JSON*

### 4. âœ… Test della Configurazione

Dopo aver inserito la configurazione:
- ğŸ§ª Clicca su "Test" per verificare la connessione
- âœ… Se tutto funziona, vedrai un messaggio di successo



---

## ğŸ”§ Configurazione degli Agenti

### 1. ğŸ› ï¸ Creazione di Funzioni e Tool

Per iniziare, creiamo una funzione dummy per verificare che il sistema funzioni.

ğŸ“ Nell'interfaccia AutoGen Studio: Gallery â†’ Tools â†’ "+ Add Tool"


![Dummy Function Creation](assets/agent_dummyFunctoon.png)
*Figura: Processo di creazione di una funzione dummy per test - mostra l'interfaccia di inserimento del codice JSON*

### 2. ğŸ¤– Assegnazione dei Modelli agli Agenti

Dopo aver creato i tool, configura gli agenti per utilizzare i modelli Ollama precedentemente configurati.

### 3. ğŸ” Pipeline di Deep Research

Ecco come appare una pipeline completa di ricerca approfondita:

![DeepResearch LAM Pipeline](assets/deepResearch_Agent.png)
*Figura: Pipeline completa del DeepResearch LAM Agent - mostra la configurazione multi-agente per ricerca approfondita*

---

## ğŸŒ Creazione della Pipeline Google LAM

### ğŸ“‹ Panoramica della Pipeline

La pipeline Google LAM rappresenta il cuore del sistema, combinando capacitÃ  di ricerca web con ragionamento avanzato. Questa sezione ti guiderÃ  nella creazione di una pipeline completa.

### ğŸ”§ Configurazione della Pipeline

#### Passo 1: Struttura Multi-Agente
La pipeline Google LAM utilizza una struttura a piÃ¹ agenti specializzati:

- **ğŸ” Search Agent:** Responsabile delle ricerche web
- **ğŸ“Š Analysis Agent:** Analizza e filtra i risultati
- **ğŸ“ Summary Agent:** Crea riassunti e risposte finali

#### Passo 2: Workflow della Pipeline
1. **Input Processing:** L'utente inserisce una query
2. **Search Execution:** Il Search Agent esegue ricerche mirate
3. **Data Analysis:** L'Analysis Agent processa i risultati
4. **Response Generation:** Il Summary Agent formula la risposta finale

#### Passo 3: Configurazione Pratica
```json
{
  "name": "Google LAM Pipeline",
  "description": "Pipeline avanzata per ricerca e analisi web",
  "agents": [
    {
      "role": "search_specialist",
      "model": "Qwen3 Model",
      "tools": ["Google Search Tool"],
      "instructions": "Esegui ricerche web mirate e raccogli informazioni rilevanti"
    },
    {
      "role": "analyst",
      "model": "Qwen3 Model", 
      "instructions": "Analizza i risultati di ricerca e identifica informazioni chiave"
    },
    {
      "role": "summarizer",
      "model": "Qwen3 Model",
      "instructions": "Crea riassunti completi e risposte strutturate"
    }
  ]
}
```

#### ğŸ¯ Vantaggi della Pipeline Google LAM
- **ğŸš€ Ricerca Intelligente:** Utilizza algoritmi avanzati per query ottimizzate
- **ğŸ§  Ragionamento Contestuale:** Comprende il contesto delle richieste
- **ğŸ“ˆ ScalabilitÃ :** Gestisce query semplici e complesse
- **ğŸ”„ Feedback Loop:** Migliora le risposte basandosi sui risultati precedenti

---

## ğŸ“š Risorse Aggiuntive

- ğŸ“– [Documentazione AutoGen](https://github.com/microsoft/autogen)
- ğŸ™ [Repository Ollama](https://github.com/ollama/ollama)


---

## Author
`Lorenzo Molfetta`

contacts: lorenzo.molfetta@unibo.it